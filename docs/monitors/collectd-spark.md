<!--- GENERATED BY gomplate from scripts/docs/monitor-page.md.tmpl --->

# collectd/spark

 Collects metrics about a Spark cluster using the
[collectd Spark Python plugin](https://github.com/signalfx/collectd-spark).
Also see
https://github.com/signalfx/integrations/tree/master/collectd-spark.

You have to specify distinct monitor configurations and discovery rules for
master and worker processes.  For the master configuration, set `isMaster`
to true.

We only support HTTP endpoints for now.


Monitor Type: `collectd/spark`

[Monitor Source Code](https://github.com/signalfx/signalfx-agent/tree/master/internal/monitors/collectd/spark)

**Accepts Endpoints**: **Yes**

**Multiple Instances Allowed**: Yes

## Configuration

| Config option | Required | Type | Description |
| --- | --- | --- | --- |
| `host` | **yes** | `string` |  |
| `port` | **yes** | `integer` |  |
| `name` | no | `string` |  |
| `isMaster` | no | `bool` | Set to `true` when monitoring a master Spark node (**default:** `false`) |
| `clusterType` | no | `string` | Should be one of `Standalone` or `Mesos` |
| `collectApplicationMetrics` | no | `bool` |  (**default:** `false`) |
| `enhancedMetrics` | no | `bool` |  (**default:** `false`) |
| `metricsToInclude` | no | `list of string` |  |
| `metricsToExclude` | no | `list of string` |  |






